{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2158,
     "status": "ok",
     "timestamp": 1694483512842,
     "user": {
      "displayName": "Bea Brolagda",
      "userId": "01319155302746720980"
     },
     "user_tz": -480
    },
    "id": "57ka2utR2576",
    "outputId": "5f273a1d-8414-4e58-8ea2-685c41ae958e"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\patri\\OneDrive\\Desktop\\files\\projects\\Web Scraper\\web-scraper-service\\ipynb-backup\\scraper-fn.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/patri/OneDrive/Desktop/files/projects/Web%20Scraper/web-scraper-service/ipynb-backup/scraper-fn.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrequests\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/patri/OneDrive/Desktop/files/projects/Web%20Scraper/web-scraper-service/ipynb-backup/scraper-fn.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbs4\u001b[39;00m \u001b[39mimport\u001b[39;00m BeautifulSoup\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/patri/OneDrive/Desktop/files/projects/Web%20Scraper/web-scraper-service/ipynb-backup/scraper-fn.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcsv\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "url = 'https://www.flipkart.com/search?q=laptop&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&sort=relevance'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "product_listings = soup.find_all('div', class_='_2kHMtA')\n",
    "\n",
    "with open('laptops.csv', mode='w', encoding='utf-8', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Price', 'Rating', 'Link'])\n",
    "\n",
    "    # Loop through each product listing and write its details to the CSV file\n",
    "    for product in product_listings:\n",
    "        title = product.find('div', class_='_4rR01T').text.strip()\n",
    "        price = product.find('div', class_='_30jeq3 _1_WHN1').text.strip()[1:].replace(',', '')\n",
    "        rating = product.find('div', class_='_3LWZlK')\n",
    "        rating = rating.text.strip() if rating else ''\n",
    "        link = 'https://www.flipkart.com' + product.find('a', class_='_1fQZEK')['href']\n",
    "        writer.writerow([title, price, rating, link])\n",
    "\n",
    "print('Scraping completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wq3FnDt5bn8O"
   },
   "source": [
    "# This scrape latest news from uk news\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_S5sbQJqYjlm"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the website to scrape\n",
    "url = 'https://www.bbc.co.uk/news'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all news articles on the page\n",
    "articles = soup.find_all('div', class_='gs-c-promo')\n",
    "\n",
    "# Find the site name\n",
    "site_name = soup.find('meta', property='og:site_name')['content']\n",
    "\n",
    "# Create a CSV file and write the header\n",
    "with open('news_data.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Title', 'Name', 'Description', 'Link'])\n",
    "\n",
    "    # Iterate over the articles and extract relevant information\n",
    "    for article in articles:\n",
    "        # Extract the title\n",
    "        title = article.find('h3').text.strip()\n",
    "\n",
    "        # Extract the description (if available)\n",
    "        description_element = article.find('p')\n",
    "        description = description_element.text.strip() if description_element else ''\n",
    "\n",
    "        # Extract the link\n",
    "        link = url + article.find('a')['href']\n",
    "\n",
    "        # Write the row to the CSV file\n",
    "        writer.writerow([title, site_name, description, link])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxLAEh7DchdS"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the website to scrape\n",
    "url = 'https://newsinfo.inquirer.net/'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all news articles on the page\n",
    "articles = soup.find_all('div', id='ncg-info')\n",
    "\n",
    "# Find the site name (if available)\n",
    "site_name_element = soup.find('meta', property='og:title')\n",
    "site_name = site_name_element['content'] if site_name_element else ''\n",
    "\n",
    "# Create a CSV file and write the header\n",
    "with open('news_data.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Title', 'Name', 'Description', 'Link'])\n",
    "\n",
    "    # Iterate over the articles and extract relevant information\n",
    "    for article in articles:\n",
    "        # Extract the title\n",
    "        title_element = article.find('h1').find('a')\n",
    "        title = title_element.text.strip() if title_element else ''\n",
    "\n",
    "        # Extract the description (if available)\n",
    "        description = ''  # No description available in the provided HTML\n",
    "\n",
    "        # Extract the link\n",
    "        link = title_element['href'] if title_element else ''\n",
    "\n",
    "        # Write the row to the CSV file\n",
    "        writer.writerow([title, site_name, description, link])\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
